{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling Udacity Project\n",
    "###### Muhammad Ariq Farhansyah Mutyara\n",
    "## Introduction\n",
    "The dataset that will be wrangled (and analyzing and visualizing) is the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. These ratings almost always have a denominator of 10. The numerators, though? Almost always greater than 10. 11/10, 12/10, 13/10, etc. Why? Because \"they're good dogs Brent.\" WeRateDogs has over 4 million followers and has received international media coverage.\n",
    "\n",
    "### Gather\n",
    "The data that we gather consists of three different dataset that is:\n",
    "- The WeRateDogs Twitter archive which downloaded manually with the name of `twitter_archive_enhanced.csv`\n",
    "- The tweet image predictions which is hosted on Udacity's servers and downloaded programmatically using the Requests library with the name of `image_predictions.tsv`\n",
    "- Scraping from Twitter API using Python's Tweepy library and storing each tweet's entire set of JSON data in a file with the name of `tweet_json.txt`. And read the file into pandas DataFrame with just tweet ID, retweet count, and favorite count column\n",
    "\n",
    "### Assess\n",
    "After gathering each of the above pieces of data, we assess it visually and programmatically. Visually, we just print entire dataset in jupyter notebook to see is there any kind of issue in the dataset. Programmatically, we use `info()` to see the datatypes and missing value, also we use `duplicated()` in some columns to see wheter there is duplicated row or not. The issue we get from assessing the 3 datasets we gathered before are:\n",
    "\n",
    "#### Quality\n",
    "##### `twitter_archive` table\n",
    "- in_reply_to_status_id and in_reply_to_user_id column have many null value\n",
    "- retweeted_status_id, retweeted_status_user_id, and retweeted_status_timestamp column have many null value\n",
    "- incorrect data types for timestamp column\n",
    "- unnecessary duplicated source column\n",
    "- weird dog name with something like a, an, the, this and more\n",
    "- some missing value in expanded_urls column\n",
    "\n",
    "##### `image_pred` table\n",
    "- there are 66 rows in jpg_url that are duplicated\n",
    "- p1, p2, p3 should be merged and make it to two columns as dog name and confidence rate\n",
    "- inconsistent name classification in p1, p2, and p3 sometimes lower case sometimes upper case\n",
    "\n",
    "#### Tidiness\n",
    "- doggo, floofer, pupper, puppo should be merged into one column in `twitter_archive` table\n",
    "- all separated dataframe should be merged into one complete dataframe\n",
    "\n",
    "### Clean\n",
    "We separate this section into three parts, that is Define, Code, and Test. Define part is where we define the process with some sort of words or pseudo code to make it clear what we want to do with the issue. Code part is where we translate the process from words to the code itself. We use python programming, pandas and and other library to code. Lastly, we test and see whether our code is working correctly or not. The clean part is very challanging one where we get a lot of trial and error to get to the point where our code and cleaning process works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
